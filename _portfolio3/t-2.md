---
title: "N-gram "
excerpt: "Statistical tests to be used for A/B testing"
collection: portfolio3
---

# Detailed Overview of Smoothing Techniques in N-Gram Language Models

In the field of Natural Language Processing (NLP), smoothing techniques are crucial for handling the issue of zero probabilities in language models, particularly in N-gram models. This article provides a detailed exploration of various smoothing techniques, their mathematical foundations, and practical examples to illustrate their application.

## Introduction to Smoothing

Smoothing techniques are used to adjust the maximum likelihood estimates of N-gram probabilities to account for unseen N-grams in the training data. This ensures that the model can handle novel sequences of words gracefully by assigning them non-zero probabilities.

## 1. Laplace (Add-One) Smoothing

Laplace smoothing, also known as add-one smoothing, is the simplest form of smoothing. It adds one to each count to avoid zero probabilities.

**Formula**:  
For a bigram model, the probability \\( P_{\text{Laplace}} \\) is calculated as:

\\[
P_{\text{Laplace}}(w_n|w_{n-1}) = \frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V}
\\]

where:
- \\( C(w_{n-1}, w_n) \\) is the count of the bigram \\( (w_{n-1}, w_n) \\),
- \\( C(w_{n-1}) \\) is the count of the preceding word,
- \\( V \\) is the size of the vocabulary.

**Example**:  
Consider a corpus with the following bigram counts:
- "I love": 1
- "love programming": 1
- "programming is": 0

With a vocabulary size \\( V \\) of 4 (I, love, programming, is), the Laplace smoothed probability of "programming is" becomes:

\\[
P_{\text{Laplace}}(\text{is}|\text{programming}) = \frac{0 + 1}{1 + 4} = \frac{1}{5} = 0.2
\\]

## 2. Add-K Smoothing

Add-K smoothing is a generalization of add-one smoothing where a fractional count \\( k \\) is added instead of one.

**Formula**:  
\\[
P_{\text{Add-K}}(w_n|w_{n-1}) = \frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + kV}
\\]

where \\( k \\) is a small positive constant.

**Example**:  
Using the same corpus and setting \\( k = 0.5 \\):

\\[
P_{\text{Add-K}}(\text{is}|\text{programming}) = \frac{0 + 0.5}{1 + 0.5 \times 4} = \frac{0.5}{3} = 0.167
\\]

## 3. Good-Turing Smoothing

Good-Turing smoothing adjusts the probability of N-grams based on the frequency of frequencies, effectively redistributing some probability mass from frequent to less frequent and unseen N-grams.

**Formula**:  
\\[
P_{\text{GT}}(w_n|w_{n-1}) = \frac{(C(w_{n-1}, w_n) + 1) \cdot N_{C+1}}{N_C \cdot C(w_{n-1})}
\\]

where:
- \\( N_C \\) is the number of N-grams with count \\( C \\).

**Example**:  
If a bigram "I love" has a count of 2, and there are 10 bigrams with a count of 1 in the training corpus, the Good-Turing smoothed probability is calculated by redistributing the probabilities based on these counts.

## 4. Kneser-Ney Smoothing

Kneser-Ney smoothing is an advanced technique that not only discounts the counts but also redistributes the probability mass based on the likelihood of a word being a novel continuation.

**Formula**:  
The bigram probability in interpolated Kneser-Ney smoothing is:

\\[
P_{\text{KN}}(w_i|w_{i-1}) = \frac{\max(C(w_{i-1}w_i) - d, 0)}{C(w_{i-1})} + \lambda(w_{i-1})P_{\text{CONT}}(w_i)
\\]

where:
- \\( d \\) is a discount constant,
- \\( P_{\text{CONT}}(w_i) \\) is the continuation probability of \\( w_i \\),
- \\( \lambda(w_{i-1}) \\) is a normalizing constant to ensure the probabilities sum to 1.

**Continuation Probability**:  
\\[
P_{\text{CONT}}(w) = \frac{| \{ v : C(vw) > 0 \} |}{\sum_{w'} | \{ v : C(vw') > 0 \} |}
\\]

**Example**:  
For a bigram "reading glasses", if "glasses" follows many different words, it will have a higher continuation probability compared to a word like "Kong", which mainly follows "Hong".

## Practical Context and Usage

**Laplace Smoothing** is often used in text classification tasks where simplicity is preferred over precision.

**Add-K Smoothing** is a fine-tuned version of Laplace smoothing, used when a more balanced approach is needed.

**Good-Turing Smoothing** is valuable in speech recognition and other applications where accurate estimation of low-frequency events is crucial.

**Kneser-Ney Smoothing** is particularly effective in machine translation and large-scale language models where capturing the diversity of contexts is important.

## Conclusion

Smoothing techniques are essential for building robust N-gram language models. Each method has its strengths and applications, from simple Laplace smoothing to the more sophisticated Kneser-Ney smoothing. Understanding and applying these techniques ensures better handling of unseen data, leading to more accurate and reliable language models.

For a more detailed study, please refer to the NLP book, which offers an in-depth analysis and additional examples of these smoothing techniques.
"""

