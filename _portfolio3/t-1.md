---
title: "Statistical A/B Testing"
excerpt: 
collection: portfolio3
---

## A/B Testing

A/B testing stands as a paramount methodology in the realm of controlled experiments, particularly in enhancing web marketing initiatives. This technique empowers decision-makers to select the most effective website design by analyzing the performance metrics of two distinct alternatives, labeled as A and B.

A/B testing involves presenting two variant designs, A and B, to website visitors in a random manner. Subsequent to their interaction, data regarding their engagement is amassed through web analytics tools. This data serves as a foundation for applying statistical tests to ascertain if there exists a significant difference in performance between the two designs.

The assessment of a website's performance can be quantified using diverse metrics. Discrete or binomial metrics, which only assume the values 0 and 1, include:                       

* Click-through rate: The likelihood of a user clicking on an advertisement.                   
* Conversion rate: The probability of a user, upon seeing an advertisement, becoming a customer.            
* Bounce rate: The chance of a user navigating to a different page within the same website after their initial visit.                             

Conversely, continuous or non-binomial metrics afford a range of values beyond binary outcomes. Notable examples of continuous metrics encompass:     

* Average revenue per user: The monthly revenue contribution per user.
* Average session duration: The typical length of a user's visit on the website.
* Average order value: The average total value of a user's order.

### Statistical Significance

To discern the relative success of designs A and B, merely comparing average values falls short of capturing the statistical significance of observed differences. It's crucial to evaluate the probability that these differences could have occurred by chance.

This evaluation involves conducting a two-sample hypothesis test. The null hypothesis (H0) posits that designs A and B are equally effective, gauged by metrics such as click-through rate or average revenue per user. The level of statistical significance is determined by the p-value, representing the likelihood of encountering a discrepancy between our samples as substantial as or more than our actual observation.

Selecting the appropriate alternative hypothesis (Ha) necessitates deciding between one-tailed and two-tailed tests. Given the absence of preliminary bias towards either design, a two-tailed test is recommended. This approach assumes Ha to embody the proposition that designs A and B differ in effectiveness.

The computation of the p-value, indicative of the probability of observing such disparities by chance, is contingent upon the underlying data distribution. We will explore how to calculate this for both discrete and continuous metrics.
